import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import kagglehub

# 1. Завантаження датасету
df = pd.read_csv("/content/data.csv")
print("Перші 5 рядків датасету:")
print(df.head())
print("\nІнформація про датасет:")
print(df.info())

# 2. Аналіз і підготовка даних
print("\nСтатистичний опис даних:")
print(df.describe())

# Перевірка на наявність пропущених значень
print("\nКількість пропущених значень:")
print(df.isnull().sum())

# Перевірка балансу класів
print("\nРозподіл цільової змінної:")
print(df['Cured'].value_counts())

# Візуалізація балансу класів
plt.figure(figsize=(6, 4))
sns.countplot(x='Cured', data=df)
plt.title('Розподіл класів (Cured)')
plt.show()

# 3. Поділ датасету на навчальну і тестову вибірки, масштабування
X = df.drop('Cured', axis=1).values
y = df['Cured'].values

# Розділення на тренувальний та тестовий набори
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Масштабування даних
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 4. Формування тензорів
X_train_tensor = torch.FloatTensor(X_train_scaled)
y_train_tensor = torch.FloatTensor(y_train).reshape(-1, 1)
X_test_tensor = torch.FloatTensor(X_test_scaled)
y_test_tensor = torch.FloatTensor(y_test).reshape(-1, 1)

# 5. Побудова нейронної мережі (логістична регресія)
class LogisticRegressionNN(nn.Module):
    def __init__(self, input_dim):
        super(LogisticRegressionNN, self).__init__()
        self.linear = nn.Linear(input_dim, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        out = self.linear(x)
        out = self.sigmoid(out)
        return out

input_dim = X_train_scaled.shape[1]
model = LogisticRegressionNN(input_dim)

# Визначення функції втрат та оптимізатора
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# 6. Навчання моделі
epochs = 1000
losses = []

for epoch in range(epochs):
    # Forward pass
    outputs = model(X_train_tensor)
    loss = criterion(outputs, y_train_tensor)

    # Backward pass and optimization
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    losses.append(loss.item())

    if (epoch+1) % 100 == 0:
        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')

# Візуалізація функції втрат
plt.figure(figsize=(8, 5))
plt.plot(losses)
plt.title('Функція втрат під час навчання')
plt.xlabel('Епоха')
plt.ylabel('Втрати')
plt.show()

# 7. Оцінка точності та функції втрат
with torch.no_grad():
    # Обчислення точності на тренувальних даних
    train_outputs = model(X_train_tensor)
    train_predicted = (train_outputs >= 0.5).float()
    train_accuracy = (train_predicted == y_train_tensor).float().mean()

    # Обчислення точності на тестових даних
    test_outputs = model(X_test_tensor)
    test_predicted = (test_outputs >= 0.5).float()
    test_accuracy = (test_predicted == y_test_tensor).float().mean()

    # Обчислення втрат на тестових даних
    test_loss = criterion(test_outputs, y_test_tensor)

print(f'\nТочність на тренувальному наборі: {train_accuracy.item():.4f}')
print(f'Точність на тестовому наборі: {test_accuracy.item():.4f}')
print(f'Втрати на тестовому наборі: {test_loss.item():.4f}')

# 8. Побудова логістичної регресії за допомогою методів МНК (sklearn)
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train_scaled, y_train)

# 9. Оцінка метрик для моделі sklearn
y_pred = log_reg.predict(X_test_scaled)
sklearn_accuracy = accuracy_score(y_test, y_pred)

print(f'\nТочність моделі sklearn: {sklearn_accuracy:.4f}')

# 10. Виведення звітів для обох моделей
print("\nЗвіт класифікації для нейронної мережі:")
y_pred_nn = test_predicted.numpy().flatten()
print(classification_report(y_test, y_pred_nn))

print("\nМатриця плутанини для нейронної мережі:")
cm_nn = confusion_matrix(y_test, y_pred_nn)
sns.heatmap(cm_nn, annot=True, fmt='d', cmap='Blues')
plt.title('Матриця плутанини (нейронна мережа)')
plt.show()

print("\nЗвіт класифікації для моделі sklearn:")
print(classification_report(y_test, y_pred))

print("\nМатриця плутанини для моделі sklearn:")
cm_sklearn = confusion_matrix(y_test, y_pred)
sns.heatmap(cm_sklearn, annot=True, fmt='d', cmap='Blues')
plt.title('Матриця плутанини (sklearn)')
plt.show()

# 11. Порівняння результатів
print("\nПорівняння результатів:")
print(f"Точність нейронної мережі: {test_accuracy.item():.4f}")
print(f"Точність sklearn: {sklearn_accuracy:.4f}")

# 12. Висновки
print("\nВисновки:")
print("1. Обидві моделі (нейронна мережа та логістична регресія з sklearn) демонструють схожу точність.")
print("2. Нейронна мережа реалізує ту саму логістичну регресію, але з використанням PyTorch.")
print("3. В обох випадках ми бачимо досить високу точність передбачення.")
print("4. Матриці плутанини для обох моделей виглядають дуже схожими, що підтверджує їх подібну ефективність.")
print("5. Використання PyTorch дає більше гнучкості для подальшого розширення моделі (наприклад, додавання прихованих шарів).")
print("6. Для простої логістичної регресії sklearn пропонує більш простий і швидкий спосіб отримати результати.")
